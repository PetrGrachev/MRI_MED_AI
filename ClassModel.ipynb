{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4240150,"sourceType":"datasetVersion","datasetId":2498912},{"sourceId":9530266,"sourceType":"datasetVersion","datasetId":5803809}],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, random_split\nfrom torch.utils.data import DataLoader\nfrom torchvision.models import resnet18\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.utils import make_grid\nimport torchvision.transforms.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pydicom as dicom\nimport matplotlib.pyplot as plt\nfrom skimage.io import imread\nimport os\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom PIL import Image\nimport cv2\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm, tqdm_notebook\nfrom PIL import Image\nfrom pathlib import Path\nfrom torchvision import transforms as tfs\nfrom multiprocessing.pool import ThreadPool\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\n\nfrom matplotlib import colors, pyplot as plt\n%matplotlib inline\n\n\nimport warnings\nwarnings.filterwarnings(action='ignore', category=DeprecationWarning)\nfrom collections import defaultdict","metadata":{"execution":{"iopub.status.busy":"2024-10-19T15:34:56.067090Z","iopub.execute_input":"2024-10-19T15:34:56.067750Z","iopub.status.idle":"2024-10-19T15:35:02.930847Z","shell.execute_reply.started":"2024-10-19T15:34:56.067709Z","shell.execute_reply":"2024-10-19T15:35:02.929903Z"},"trusted":true},"outputs":[{"name":"stderr","text":"US1_J2KR.dcm:   0%|          | 38.0/154k [00:00<01:55, 1.33kB/s]\nMR-SIEMENS-DICOM-WithOverlays.dcm:   0%|          | 125/511k [00:00<03:02, 2.81kB/s]\nOBXXXX1A.dcm:   0%|          | 119/486k [00:00<03:01, 2.68kB/s]\nUS1_UNCR.dcm:   0%|          | 226/923k [00:00<03:50, 4.01kB/s]\ncolor3d_jpeg_baseline.dcm:   0%|          | 1.50k/6.14M [00:00<06:34, 15.6kB/s]\n/opt/conda/lib/python3.10/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.18 (you have 1.4.15). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n  check_for_updates()\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/input/medicalnet/MedicalNet/MedicalNet')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T15:31:39.682724Z","iopub.execute_input":"2024-10-19T15:31:39.683107Z","iopub.status.idle":"2024-10-19T15:31:39.687501Z","shell.execute_reply.started":"2024-10-19T15:31:39.683072Z","shell.execute_reply":"2024-10-19T15:31:39.686581Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"!pip install rasterio","metadata":{"execution":{"iopub.status.busy":"2024-10-19T15:35:11.916826Z","iopub.execute_input":"2024-10-19T15:35:11.917766Z","iopub.status.idle":"2024-10-19T15:35:26.071353Z","shell.execute_reply.started":"2024-10-19T15:35:11.917724Z","shell.execute_reply":"2024-10-19T15:35:26.070222Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting rasterio\n  Downloading rasterio-1.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\nCollecting affine (from rasterio)\n  Downloading affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\nRequirement already satisfied: attrs in /opt/conda/lib/python3.10/site-packages (from rasterio) (23.2.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from rasterio) (2024.8.30)\nRequirement already satisfied: click>=4.0 in /opt/conda/lib/python3.10/site-packages (from rasterio) (8.1.7)\nRequirement already satisfied: cligj>=0.5 in /opt/conda/lib/python3.10/site-packages (from rasterio) (0.7.2)\nRequirement already satisfied: numpy>=1.24 in /opt/conda/lib/python3.10/site-packages (from rasterio) (1.26.4)\nRequirement already satisfied: click-plugins in /opt/conda/lib/python3.10/site-packages (from rasterio) (1.1.1)\nRequirement already satisfied: pyparsing in /opt/conda/lib/python3.10/site-packages (from rasterio) (3.1.2)\nDownloading rasterio-1.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading affine-2.4.0-py3-none-any.whl (15 kB)\nInstalling collected packages: affine, rasterio\nSuccessfully installed affine-2.4.0 rasterio-1.4.1\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport random\nfrom glob import glob\nimport os, shutil\nfrom tqdm import tqdm\ntqdm.pandas()\nimport time\nimport copy\nimport joblib\nfrom collections import defaultdict\nimport gc\nfrom IPython import display as ipd\n\n# visualization\nfrom PIL import Image\nimport cv2\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\n\n# Sklearn\nfrom sklearn.model_selection import StratifiedKFold, KFold, StratifiedGroupKFold\n\n# PyTorch \nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda import amp\n\nimport timm\n\n# Albumentations for augmentations\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport rasterio\nfrom joblib import Parallel, delayed\n\n# For colored terminal text\nfrom colorama import Fore, Back, Style\nc_  = Fore.GREEN\nsr_ = Style.RESET_ALL\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# For descriptive error messages\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n\nimport pydicom as dicom\nimport nibabel as nib\nimport sys\nimport glob\nimport os\nimport numpy as np\nimport h5py\nfrom sklearn.preprocessing import StandardScaler","metadata":{"execution":{"iopub.status.busy":"2024-10-19T15:35:26.073346Z","iopub.execute_input":"2024-10-19T15:35:26.073691Z","iopub.status.idle":"2024-10-19T15:35:28.072238Z","shell.execute_reply.started":"2024-10-19T15:35:26.073657Z","shell.execute_reply":"2024-10-19T15:35:28.071337Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda import amp\n\nimport timm\nimport tifffile as tiff","metadata":{"execution":{"iopub.status.busy":"2024-10-19T15:35:28.074010Z","iopub.execute_input":"2024-10-19T15:35:28.075115Z","iopub.status.idle":"2024-10-19T15:35:28.081339Z","shell.execute_reply.started":"2024-10-19T15:35:28.075067Z","shell.execute_reply":"2024-10-19T15:35:28.080266Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def read_labels_from_file(file_path):\n    values = []\n    with open(file_path, 'r', encoding='utf-8') as f:\n        lines = f.readlines()\n        for line in lines:\n            try:\n                value = int(line.split(': ')[1])\n            except ValueError:\n                value = line.split(': ')[1].strip()  # Если значение не может быть преобразовано в int, оставляем его как строку\n            if str(value) in ['3а'  ,'3','3a']:\n                value = '3a'\n            values.append(value)\n    return values","metadata":{"execution":{"iopub.status.busy":"2024-10-19T15:35:28.083696Z","iopub.execute_input":"2024-10-19T15:35:28.084110Z","iopub.status.idle":"2024-10-19T15:35:28.095821Z","shell.execute_reply.started":"2024-10-19T15:35:28.084064Z","shell.execute_reply":"2024-10-19T15:35:28.094818Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def load_tif_mask(x_path, y_path, mask_pth, num_imgs=10,img_size=512):\n    '''\n    Перобразует tif в набор срезов с окном num_imgs на выходе  [num_imgs,img_size,img_size]\n\n    x_path - путь до снимка \n    y_path - путь до меток \n    num_imgs - окно для взятия снимка\n    img_size - размер итоговй снимка \n    '''\n    images = []\n    labels = []\n    image =  tiff.imread(x_path)\n    # mask = Image.open(mask_pth)\n    i = 0\n    # image = [np.fliplr(np.array(img)) for img in image]\n    middle = len(image)//2\n    num_imgs2 = num_imgs//2\n\n    p1 = max(0, middle - num_imgs2)\n    p2 = min(len(image), middle + num_imgs2)\n\n    images = np.array(image[p1:p2])\n\n    # while len(images)<10:\n        # try:\n        #     mask.seek(i)\n        #     mask_array = np.array(mask)\n        #     image.seek(i)\n        #     image_array = np.array(image)\n        #     if sum(sum(mask_array)) > 0:\n        #         images.append(image_array)\n        #         labels.append(read_labels_from_file(y_path))\n        #     i += 1\n        # except EOFError:\n        #     break\n\n    if len(images)<num_imgs:\n        \n        n_zero = np.zeros((num_imgs - images.shape[0],img_size, img_size))\n        images = np.concatenate((images,  n_zero), axis =0)\n\n    labels = read_labels_from_file(y_path)\n    return images, labels","metadata":{"execution":{"iopub.status.busy":"2024-10-19T15:35:28.097168Z","iopub.execute_input":"2024-10-19T15:35:28.097633Z","iopub.status.idle":"2024-10-19T15:35:28.108312Z","shell.execute_reply.started":"2024-10-19T15:35:28.097553Z","shell.execute_reply":"2024-10-19T15:35:28.107325Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def load_tif_mask_inv(x_path, y_path, mask_pth, num_imgs=10,img_size=512):\n    '''\n    Перобразует tif в инвертирвоанный набор срезов с окном num_imgs на выходе  [num_imgs,img_size,img_size]\n\n    x_path - путь до снимка \n    y_path - путь до меток \n    num_imgs - окно для взятия снимка\n    img_size - размер итоговй снимка \n    '''\n    images = []\n    labels = []\n    image =  tiff.imread(x_path)\n    image = [np.fliplr(np.array(img)) for img in image]\n    # mask = Image.open(mask_pth)\n    i = 0\n\n    middle = len(image)//2\n    num_imgs2 = num_imgs//2\n\n    p1 = max(0, middle - num_imgs2)\n    p2 = min(len(image), middle + num_imgs2)\n\n    images = np.array(image[p1:p2])\n\n\n    if len(images)<num_imgs:\n        \n        n_zero = np.zeros((num_imgs - images.shape[0],img_size, img_size))\n        images = np.concatenate((images,  n_zero), axis =0)\n\n            \n    labels = read_labels_from_file(y_path)[::-1]\n\n    return images, labels","metadata":{"execution":{"iopub.status.busy":"2024-10-19T15:35:28.109769Z","iopub.execute_input":"2024-10-19T15:35:28.110122Z","iopub.status.idle":"2024-10-19T15:35:28.125059Z","shell.execute_reply.started":"2024-10-19T15:35:28.110079Z","shell.execute_reply":"2024-10-19T15:35:28.124070Z"},"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def get_pathes_mask(path):\n    '''\n    Возвращает массивы с путясми снимков и метками класса ( для снимков где есть маска)\n\n    path - путь до директории со снимками \n    '''\n    x_pathes_all = []\n    y_pathes_all = []\n    for patient in os.listdir(path):\n            x_pathes = []\n            y_pathes = []\n        \n            for ID_s in os.listdir(path + '/'+ patient ):\n                if 'ID' in ID_s:\n                    msk_t1 = 0\n                    msk_t2 = 0  \n                    for tif_name in os.listdir(path + '/'+ patient + '/'+ID_s):\n                        if 'Cor' in tif_name:\n                            if 'T1' in tif_name:\n                \n                                if  'mask' not in tif_name.lower():\n                                    x_pathes.append(path + '/'+ patient + '/' + ID_s + '/'+ tif_name)\n\n                                elif 'mask' in tif_name.lower():\n                                    msk_t1 = 1\n                                    y_pathes.append([path + '/'+ patient + '/'+ID_s + '/' + tif_name,path + '/'+ patient + '/' +'labels.txt'])\n\n                            elif 'T2' in tif_name:\n                                \n                                if 'mask' not in tif_name.lower():\n                                    x_pathes.append(path + '/'+ patient + '/'+ID_s + '/' + tif_name)\n                                \n                                elif 'mask' in tif_name.lower():\n                                    msk_t2 = 1\n                                    y_pathes.append([path + '/'+ patient + '/'+ID_s + '/' + tif_name,path + '/'+ patient + '/' +'labels.txt'])\n                            \n                    if msk_t1==0:\n                        x_pathes.pop()\n                    if msk_t2==0:\n                        x_pathes.pop()\n                            \n            x_pathes_all.append(x_pathes)\n            y_pathes_all.append(y_pathes)\n    return x_pathes_all, y_pathes_all","metadata":{"execution":{"iopub.status.busy":"2024-10-19T15:35:28.586433Z","iopub.execute_input":"2024-10-19T15:35:28.587411Z","iopub.status.idle":"2024-10-19T15:35:28.601965Z","shell.execute_reply.started":"2024-10-19T15:35:28.587360Z","shell.execute_reply":"2024-10-19T15:35:28.601068Z"},"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def get_pathes(path):\n    '''\n    Возвращает массивы с путясми снимков и метками класса \n\n    path - путь до директории со снимками \n    '''\n    x_pathes_all = []\n    y_pathes_all = []\n    for patient in os.listdir(path):\n        x_pathes = []\n        y_pathes = []\n        \n        for ID_s in os.listdir(path + '/'+ patient ):\n            if 'ID' in ID_s:\n                for tif_name in os.listdir(path + '/'+ patient + '/'+ID_s):\n    \n                    if 'Cor' in tif_name:\n                        if 'T1' in tif_name:\n\n                            if 'mask' not in tif_name.lower():\n                                x_pathes.append(path + '/'+ patient + '/' + ID_s + '/'+ tif_name)\n                                y_pathes.append(path + '/'+ patient + '/' +'labels.txt')\n                            # elif 'mask' in tif_name.lower():\n                            # y_pathes.append(path + '/'+ patient + '/' + tif_name)\n\n                        elif 'T2' in tif_name:\n                            \n                            if 'mask' not in tif_name.lower():\n                                x_pathes.append(path + '/'+ patient + '/'+ID_s + '/' + tif_name)\n                                y_pathes.append(path + '/'+ patient + '/' +'labels.txt')\n                            # elif 'mask' in tif_name.lower():\n                            #     y_pathes.append(path + '/'+ patient + '/' + tif_name)\n                    \n                        \n        x_pathes_all.append(x_pathes)\n        y_pathes_all.append(y_pathes)\n    \n    return x_pathes_all, y_pathes_all","metadata":{"execution":{"iopub.status.busy":"2024-10-19T15:35:30.743364Z","iopub.execute_input":"2024-10-19T15:35:30.744212Z","iopub.status.idle":"2024-10-19T15:35:30.756169Z","shell.execute_reply.started":"2024-10-19T15:35:30.744140Z","shell.execute_reply":"2024-10-19T15:35:30.754843Z"},"trusted":true},"outputs":[],"execution_count":17},{"cell_type":"code","source":"file_path = os.path.join('/kaggle/input/brain-tumor-mri/data_29_05_24_cls108_seg72/ID_65', 'labels.txt')\n\nvalues = read_labels_from_file('/kaggle/input/brain-tumor-mri/data_29_05_24_cls108_seg72/ID_89/labels.txt')[::-1]\nprint(values)","metadata":{"execution":{"iopub.status.busy":"2024-10-19T15:35:33.667615Z","iopub.execute_input":"2024-10-19T15:35:33.667958Z","iopub.status.idle":"2024-10-19T15:35:33.681716Z","shell.execute_reply.started":"2024-10-19T15:35:33.667926Z","shell.execute_reply":"2024-10-19T15:35:33.680722Z"},"trusted":true},"outputs":[{"name":"stdout","text":"['3a', 4]\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"x_pth ,y_pth = get_pathes('/kaggle/input/brain-tumor-mri/data_29_05_24_cls108_seg72')","metadata":{"execution":{"iopub.status.busy":"2024-10-19T15:35:35.791682Z","iopub.execute_input":"2024-10-19T15:35:35.792309Z","iopub.status.idle":"2024-10-19T15:35:38.058138Z","shell.execute_reply.started":"2024-10-19T15:35:35.792271Z","shell.execute_reply":"2024-10-19T15:35:38.057200Z"},"trusted":true},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def flatten(xss):# рвзвертывает список спсиков в список\n    return [x for xs in xss for x in xs]","metadata":{"execution":{"iopub.status.busy":"2024-10-19T15:35:38.059598Z","iopub.execute_input":"2024-10-19T15:35:38.059909Z","iopub.status.idle":"2024-10-19T15:35:38.064422Z","shell.execute_reply.started":"2024-10-19T15:35:38.059877Z","shell.execute_reply":"2024-10-19T15:35:38.063380Z"},"trusted":true},"outputs":[],"execution_count":20},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom torch.utils.data import Dataset\n\nclass MedicalDataset(torch.utils.data.Dataset):\n    def __init__(self, file_paths, labels, transform=None):\n        self.file_paths = file_paths\n        self.labels = labels\n        self.transform = transform\n        self.knsop_labels = {'0': 0, '1': 1, '2': 2, '3a': 3, '3b': 3, '4': 4}\n\n    def __len__(self):\n        return len(self.file_paths)\n\n    def __getitem__(self, idx):\n        # Загружаем TIFF изображения и метки\n        img_data, y_train = load_tif_mask(self.file_paths[idx], self.labels[idx], self.labels[idx])\n\n        # Применение преобразований ко всем срезам (если есть)\n        if self.transform:\n            res_img = np.zeros((img_data.shape[0], 224, 224))  # Предполагаем, что выходное изображение 224x224\n            for i in range(img_data.shape[0]):  # Применяем трансформации ко всем срезам\n                transformed = self.transform(image=np.array(img_data[i], dtype=np.float32))\n                res_img[i] = transformed['image']\n\n            # Нормализуем данные\n            res_img = (res_img - np.min(res_img)) / (np.max(res_img) - np.min(res_img))\n        else:\n            # Нормализация данных без применения преобразований\n            res_img = (img_data - np.min(img_data)) / (np.max(img_data) - np.min(img_data))\n\n        # Преобразуем данные к формату [1, Depth, Height, Width]\n        img_data = np.expand_dims(res_img, axis=0)  # Добавляем канал (Channels = 1, Depth, Height, Width)\n        \n        print(f\"Image shape: {img_data.shape}\")  # Должен быть [1, Depth, 224, 224]\n        \n        Right = self.knsop_labels[str(y_train[1])]  # Используем метку для правого полушария\n\n        sample = {\n            \"image\": torch.from_numpy(img_data),  # Преобразуем в тензор\n            \"labelS\": Right  # Метка\n        }\n\n        return sample\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-19T15:35:39.424394Z","iopub.execute_input":"2024-10-19T15:35:39.424775Z","iopub.status.idle":"2024-10-19T15:35:39.436702Z","shell.execute_reply.started":"2024-10-19T15:35:39.424740Z","shell.execute_reply":"2024-10-19T15:35:39.435731Z"},"trusted":true},"outputs":[],"execution_count":21},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom torch.utils.data import Dataset\n\nclass MedicalDatasetInv(torch.utils.data.Dataset):\n    def __init__(self, file_paths, labels, transform=None):\n        self.file_paths = file_paths\n        self.labels = labels\n        self.transform = transform\n        self.knsop_labels = {'0': 0, '1': 1, '2': 2, '3a': 3, '3b': 3, '4': 4}\n\n    def __len__(self):\n        return len(self.file_paths)\n\n    def __getitem__(self, idx):\n        # Загружаем TIFF изображения и метки\n        img_data, y_train = load_tif_mask(self.file_paths[idx], self.labels[idx], self.labels[idx])\n\n        # Применение преобразований ко всем срезам (если есть)\n        if self.transform:\n            res_img = np.zeros((img_data.shape[0], 224, 224))  # Предполагаем, что выходное изображение 224x224\n            for i in range(img_data.shape[0]):  # Применяем трансформации ко всем срезам\n                transformed = self.transform(image=np.array(img_data[i], dtype=np.float32))\n                res_img[i] = transformed['image']\n\n            # Нормализуем данные\n            res_img = (res_img - np.min(res_img)) / (np.max(res_img) - np.min(res_img))\n        else:\n            # Нормализация данных без применения преобразований\n            res_img = (img_data - np.min(img_data)) / (np.max(img_data) - np.min(img_data))\n\n        # Преобразуем данные к формату [1, Depth, Height, Width]\n        img_data = np.expand_dims(res_img, axis=0)  # Добавляем канал (Channels = 1, Depth, Height, Width)\n        print(f\"Image shape: {img_data.shape}\")  # Должен быть [1, Depth, 224, 224]\n        \n        Right = self.knsop_labels[str(y_train[1])]  # Используем метку для правого полушария\n\n        sample = {\n            \"image\": torch.from_numpy(img_data),  # Преобразуем в тензор\n            \"labelS\": Right  # Метка\n        }\n\n        return sample\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T15:35:41.798856Z","iopub.execute_input":"2024-10-19T15:35:41.799556Z","iopub.status.idle":"2024-10-19T15:35:41.810554Z","shell.execute_reply.started":"2024-10-19T15:35:41.799517Z","shell.execute_reply":"2024-10-19T15:35:41.809547Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"data_transforms = {\n    \"train\": A.Compose([\n        A.augmentations.crops.transforms.CenterCrop(256,256),\n        A.Resize(224,224, interpolation=cv2.INTER_NEAREST),\n        # A.HorizontalFlip(p=0.5),\n        # A.VerticalFlip(p=0.5),\n        A.ShiftScaleRotate(shift_limit=0.0225, scale_limit=0.005, rotate_limit=10, p=1.),\n        A.OneOf([\n            A.GridDistortion(num_steps=5, distort_limit=0.05, p=1.0),\n# #             A.OpticalDistortion(distort_limit=0.05, shift_limit=0.05, p=1.0),\n            A.ElasticTransform(alpha=1, sigma=50, alpha_affine=None, p=1.0)\n        ], p=0.25),\n        A.CoarseDropout(max_holes=8, max_height=224//20, max_width=224//20,\n                         min_holes=5, fill_value=0, mask_fill_value=0, p=1.0),\n        A.augmentations.Normalize(mean=(0.485, ), std=(0.229, )),\n        ]\n        , p=1.0),\n    \n    \"valid\": A.Compose([\n        A.augmentations.crops.transforms.CenterCrop(256,256),\n        A.Resize(224,224, interpolation=cv2.INTER_NEAREST),\n        A.augmentations.Normalize(mean=(0.485, ), std=(0.229, )),\n        ], p=1.0)\n}","metadata":{"execution":{"iopub.status.busy":"2024-10-19T15:35:44.771180Z","iopub.execute_input":"2024-10-19T15:35:44.771585Z","iopub.status.idle":"2024-10-19T15:35:44.785745Z","shell.execute_reply.started":"2024-10-19T15:35:44.771550Z","shell.execute_reply":"2024-10-19T15:35:44.784582Z"},"trusted":true},"outputs":[],"execution_count":23},{"cell_type":"code","source":"class CFG:\n    seed          = 42\n    debug         = False # set debug=False for Full Training\n    exp_name      = 'Baselinev2'\n    comment       = 'unet-efficientnet_b1-224x224-aug2-split2'\n    model_name    = 'Eff'\n    backbone      = 'efficientnet-b1'\n    train_bs      = 128\n    valid_bs      = train_bs*2\n    img_size      = [224, 224]\n    epochs        = 50\n    lr            = 2e-4\n    scheduler     = 'CosineAnnealingLR'\n    min_lr        = 1e-6\n    T_max         = int(30000/train_bs*epochs)+50\n    T_0           = 25\n    warmup_epochs = 0\n    wd            = 1e-5\n    n_accumulate  = max(1, 32//train_bs)\n    n_fold        = 5\n    num_classes   = 6\n    device        = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-10-19T15:35:46.658215Z","iopub.execute_input":"2024-10-19T15:35:46.659137Z","iopub.status.idle":"2024-10-19T15:35:46.666817Z","shell.execute_reply.started":"2024-10-19T15:35:46.659086Z","shell.execute_reply":"2024-10-19T15:35:46.665616Z"},"trusted":true},"outputs":[],"execution_count":24},{"cell_type":"code","source":"import wandb\nfrom torchvision.models import resnet50, resnext50_32x4d,efficientnet_b6","metadata":{"execution":{"iopub.status.busy":"2024-10-19T15:35:48.671656Z","iopub.execute_input":"2024-10-19T15:35:48.672017Z","iopub.status.idle":"2024-10-19T15:35:48.676310Z","shell.execute_reply.started":"2024-10-19T15:35:48.671983Z","shell.execute_reply":"2024-10-19T15:35:48.675421Z"},"trusted":true},"outputs":[],"execution_count":25},{"cell_type":"code","source":"def fetch_scheduler(optimizer):\n    if CFG.scheduler == 'CosineAnnealingLR':\n        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=CFG.T_max, \n                                                   eta_min=CFG.min_lr)\n    elif CFG.scheduler == 'CosineAnnealingWarmRestarts':\n        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=CFG.T_0, \n                                                             eta_min=CFG.min_lr)\n    elif CFG.scheduler == 'ReduceLROnPlateau':\n        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer,\n                                                   mode='min',\n                                                   factor=0.1,\n                                                   patience=7,\n                                                   threshold=0.0001,\n                                                   min_lr=CFG.min_lr,)\n    elif CFG.scheduer == 'ExponentialLR':\n        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.85)\n    elif CFG.scheduler == None:\n        return None\n        \n    return scheduler","metadata":{"execution":{"iopub.status.busy":"2024-10-19T15:35:50.316561Z","iopub.execute_input":"2024-10-19T15:35:50.316937Z","iopub.status.idle":"2024-10-19T15:35:50.324212Z","shell.execute_reply.started":"2024-10-19T15:35:50.316903Z","shell.execute_reply":"2024-10-19T15:35:50.323238Z"},"trusted":true},"outputs":[],"execution_count":26},{"cell_type":"code","source":"def accuracy(y_true, y_pred):\n    \n    \"\"\"\n    Function to calculate accuracy\n    -> param y_true: list of true values\n    -> param y_pred: list of predicted values\n    -> return: accuracy score\n    \n    \"\"\"\n    correct_predictions = 0\n    for yt, yp in zip(y_true, y_pred):\n        if yt == yp:\n            correct_predictions += 1\n    return correct_predictions / len(y_true)\n\n\n\ndef recall_score_multiclass(y_true, y_pred, num_classes=6):\n    recall = np.zeros(num_classes)\n    for c in range(num_classes):\n        true_positives = ((y_true == c) & (y_pred == c)).sum().item()\n        possible_positives = (y_true == c).sum().item()\n        recall[c] = true_positives / (possible_positives + 1e-7)\n    return recall.mean()\n\ndef accuracy_score_multiclass(y_true, y_pred):\n    correct_predictions = (y_true == y_pred).sum().item()\n    accuracy = correct_predictions / y_true.numel()\n    return accuracy","metadata":{"execution":{"iopub.status.busy":"2024-10-19T15:35:52.110477Z","iopub.execute_input":"2024-10-19T15:35:52.111351Z","iopub.status.idle":"2024-10-19T15:35:52.118794Z","shell.execute_reply.started":"2024-10-19T15:35:52.111312Z","shell.execute_reply":"2024-10-19T15:35:52.117830Z"},"trusted":true},"outputs":[],"execution_count":27},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score\n\nprecision = precision_score(torch.tensor([1]),torch.tensor([1]), average='macro')\nrecall = recall_score(torch.tensor([1]),torch.tensor([1]), average='macro')\nf1 = f1_score(torch.tensor([1,3]),torch.tensor([1,2]), average='macro')\nprint(precision,recall,f1)","metadata":{"execution":{"iopub.status.busy":"2024-10-19T15:35:54.209646Z","iopub.execute_input":"2024-10-19T15:35:54.210011Z","iopub.status.idle":"2024-10-19T15:35:54.251618Z","shell.execute_reply.started":"2024-10-19T15:35:54.209976Z","shell.execute_reply":"2024-10-19T15:35:54.250749Z"},"trusted":true},"outputs":[{"name":"stdout","text":"1.0 1.0 0.3333333333333333\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n    model.train()\n    scaler = amp.GradScaler()\n    train_scores = []\n    dataset_size = 0\n    running_loss = 0.0\n    \n    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Train ')\n    for step, (sample) in pbar:  \n        images, label = sample['image'], sample['labelS']\n \n        images  = images.to(device).half() \n        label   = label.to(device).long() \n        batch_size = images.size(0)\n        \n        with amp.autocast(enabled=True):\n\n            y_pred  = model(images)\n\n            loss   = criterion(y_pred, label)\n            loss   = loss / CFG.n_accumulate\n            \n        scaler.scale(loss).backward()\n    \n        if (step + 1) % CFG.n_accumulate == 0:\n            scaler.step(optimizer)\n            scaler.update()\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            if scheduler is not None:\n                scheduler.step()\n                \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss / dataset_size\n\n        y_pred = torch.argmax(y_pred, 1)\n        \n        precision = precision_score(label.cpu().detach().numpy(), y_pred.cpu().detach().numpy(), average='macro')\n        recall = recall_score(label.cpu().detach().numpy(), y_pred.cpu().detach().numpy(), average='macro')\n        acc = accuracy(label.cpu().detach().numpy(), y_pred.cpu().detach().numpy())\n        \n        train_scores.append([precision, recall, acc])\n        \n        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n        current_lr = optimizer.param_groups[0]['lr']\n        pbar.set_postfix(train_loss=f'{epoch_loss:0.4f}',\n                        lr=f'{current_lr:0.5f}',\n                        gpu_mem=f'{mem:0.2f} GB')\n        \n    train_scores  = np.mean(train_scores, axis=0)    \n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    return epoch_loss, train_scores","metadata":{"execution":{"iopub.status.busy":"2024-10-19T15:35:55.985965Z","iopub.execute_input":"2024-10-19T15:35:55.986362Z","iopub.status.idle":"2024-10-19T15:35:55.999842Z","shell.execute_reply.started":"2024-10-19T15:35:55.986323Z","shell.execute_reply":"2024-10-19T15:35:55.998910Z"},"trusted":true},"outputs":[],"execution_count":29},{"cell_type":"code","source":"@torch.no_grad()\ndef valid_one_epoch(model, dataloader, device, epoch):\n    model.eval()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    val_scores = []\n    \n    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Valid ')\n    for step, (sample) in pbar:  \n        images, label = sample['image'], sample['labelS']\n \n        images  = images.to(device).half() \n        label   = label.to(device).long() \n        \n        batch_size = images.size(0)\n        with amp.autocast(enabled=True):\n            y_pred  = model(images)\n        \n            loss    = criterion(y_pred, label)\n        \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss / dataset_size\n        \n        y_pred = torch.argmax(y_pred, 1)\n\n        precision = precision_score(label.cpu().detach().numpy(), y_pred.cpu().detach().numpy(), average='macro')\n        recall = recall_score(label.cpu().detach().numpy(), y_pred.cpu().detach().numpy(), average='macro')\n        acc = accuracy(label.cpu().detach().numpy(), y_pred.cpu().detach().numpy())\n        \n        val_scores.append([precision, recall, acc])\n        \n        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n        current_lr = optimizer.param_groups[0]['lr']\n        pbar.set_postfix(valid_loss=f'{epoch_loss:0.4f}',\n                        lr=f'{current_lr:0.5f}',\n                        gpu_memory=f'{mem:0.2f} GB')\n        \n        \n    val_scores  = np.mean(val_scores, axis=0)\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    return epoch_loss, val_scores","metadata":{"execution":{"iopub.status.busy":"2024-10-19T15:35:57.979883Z","iopub.execute_input":"2024-10-19T15:35:57.980257Z","iopub.status.idle":"2024-10-19T15:35:57.991818Z","shell.execute_reply.started":"2024-10-19T15:35:57.980220Z","shell.execute_reply":"2024-10-19T15:35:57.990932Z"},"trusted":true},"outputs":[],"execution_count":30},{"cell_type":"code","source":"def run_training(model, optimizer, scheduler, device, num_epochs):\n    # To automatically log gradients\n    # wandb.watch(model, log_freq=100)\n    \n    if torch.cuda.is_available():\n        print(\"cuda: {}\\n\".format(torch.cuda.get_device_name()))\n    \n    start = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc     = -np.inf\n    best_epoch     = -1\n    history = defaultdict(list)\n    \n    for epoch in range(1, num_epochs + 1): \n        gc.collect()\n        print(f'Epoch {epoch}/{num_epochs}', end='')\n        train_loss,train_scores = train_one_epoch(model, optimizer, scheduler, \n                                           dataloader=train_loader, \n                                           device=CFG.device, epoch=epoch)\n        train_precision, train_recall, train_acc = train_scores\n\n        print(f'Train Acc: {train_acc:0.4f} | Train Recall: {train_recall:0.4f}')\n\n        val_loss, val_scores = valid_one_epoch(model, valid_loader, \n                                                 device=CFG.device, \n                                                 epoch=epoch)\n        val_precision, val_recall, val_acc = val_scores\n     \n        history['Train Loss'].append(train_loss)\n        history['Train Precision'].append(train_precision)\n        history['Train Recall'].append(train_recall)\n        history['Train Acc'].append(train_acc)\n\n        history['Valid Loss'].append(val_loss)\n        history['Valid Precision'].append(val_precision)\n        history['Valid Recall'].append(val_recall)\n        history['Valid Acc'].append(val_acc)\n        \n        \n        # Log the metrics\n        # wandb.log({\"Train Loss\": train_loss, \n        #            \"Valid Loss\": val_loss,\n        #            \"Valid Dice\": val_dice,\n        #            \"Valid Jaccard\": val_jaccard,\n        #            \"LR\":scheduler.get_last_lr()[0]})\n \n        print(f'Valid Acc: {val_acc:0.4f} | Valid Recall: {val_recall:0.4f}')\n        \n        # deep copy the model\n        if val_acc > best_acc:\n            print(f\"{c_}Valid Score Improved ({best_acc:0.4f} ---> {val_acc:0.4f})\")\n            best_recall   = val_recall\n            best_acc = val_acc\n            best_precision = val_precision\n            best_epoch   = epoch\n            #run.summary[\"Best Recall\"]    = best_recall\n            #run.summary[\"Best Acc\"] = best_acc\n            #run.summary[\"Best Precision\"] = best_precision\n            #run.summary[\"Best Epoch\"]   = best_epoch\n            best_model_wts = copy.deepcopy(model.state_dict())\n            PATH = f\"best_epoch-{fold:02d}.bin\"\n            torch.save(model.state_dict(), PATH)\n            # Save a model file from the current directory\n            # wandb.save(PATH)\n            print(f\"Model Saved{sr_}\")\n            \n        last_model_wts = copy.deepcopy(model.state_dict())\n        PATH = f\"last_epoch-{fold:02d}.bin\"\n        torch.save(model.state_dict(), PATH)\n            \n        print(); print()\n    \n    end = time.time()\n    time_elapsed = end - start\n    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n    print(\"Best Score: {:.4f}\".format(best_acc))\n    \n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    \n    return model, history","metadata":{"execution":{"iopub.status.busy":"2024-10-19T15:36:00.288050Z","iopub.execute_input":"2024-10-19T15:36:00.288440Z","iopub.status.idle":"2024-10-19T15:36:00.303867Z","shell.execute_reply.started":"2024-10-19T15:36:00.288403Z","shell.execute_reply":"2024-10-19T15:36:00.302249Z"},"trusted":true},"outputs":[],"execution_count":31},{"cell_type":"code","source":"class ResNMEAN(nn.Module):#Resnet 50 like feature extractor with MEAN aggregation and MLP cls\n    def __init__(self,input_dim, hidden_dim, output_dim, num_layers=1, dropout=0.1):\n        super(ResNMEAN, self).__init__()\n\n        self.model = resnet50(pretrained=True)\n        # for param in resnet.parameters():\n        #     param.requires_grad = False\n        original_conv1_weight = self.model.conv1.weight.data\n        original_conv1_weight_mean = original_conv1_weight.mean(dim=1, keepdim=True)\n        self.model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n        self.model.conv1.weight.data = original_conv1_weight_mean\n        \n        self.model.fc = nn.Identity()\n        self.fc1 = nn.Sequential(\n                                nn.Linear(hidden_dim, hidden_dim//2),\n                                nn.ReLU(),\n                                nn.Dropout(dropout),\n                                nn.Linear(hidden_dim//2, output_dim),\n                                )\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        batch_size, c, seq_length,  h, w = x.shape\n        cnn_embed_seq = []\n        for t in range(seq_length):\n            # with torch.no_grad():\n                x_t = self.model(x[:, :, t, :, :].to('cuda').float())  # pass each frame through ResNet\n                x_t = x_t.view(batch_size, -1)\n                cnn_embed_seq.append(x_t)\n            \n        cnn_embed_seq = torch.stack(cnn_embed_seq, dim=1)\n        mean_cnn_embed = cnn_embed_seq.mean(dim=1)\n        output = self.fc1(mean_cnn_embed)\n        \n        \n        return output","metadata":{"execution":{"iopub.status.busy":"2024-10-19T15:36:03.302036Z","iopub.execute_input":"2024-10-19T15:36:03.302731Z","iopub.status.idle":"2024-10-19T15:36:03.312959Z","shell.execute_reply.started":"2024-10-19T15:36:03.302688Z","shell.execute_reply":"2024-10-19T15:36:03.311981Z"},"trusted":true},"outputs":[],"execution_count":32},{"cell_type":"code","source":"\ndevice=CFG.device\nimport torch\nimport torch.nn as nn\nfrom models.resnet import resnet50\n\nclass MedicalNet(nn.Module):\n    def __init__(self, path_to_weights, device, num_classes=5):\n        super(MedicalNet, self).__init__()\n        # Инициализируем ResNet-50 с представительной глубиной\n        self.model = resnet50(\n            sample_input_D=16,  # Задайте представительную глубину ваших данных\n            sample_input_H=224,\n            sample_input_W=224,\n            num_seg_classes=num_classes\n        )\n\n        # Заменяем последний слой пулинга на адаптивный, чтобы работать с разной глубиной\n        self.model.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n\n        # Получаем количество входных признаков для линейного слоя\n        num_ftrs = self.model.conv_seg.in_features\n\n        # Заменяем последний линейный слой на новый\n        self.model.conv_seg = nn.Linear(num_ftrs, num_classes)\n\n        # Загружаем предобученные веса\n        net_dict = self.model.state_dict()\n        pretrained_weights = torch.load(path_to_weights, map_location=device)\n        pretrain_dict = {\n            k.replace(\"module.\", \"\"): v for k, v in pretrained_weights['state_dict'].items()\n            if k.replace(\"module.\", \"\") in net_dict.keys()\n        }\n        net_dict.update(pretrain_dict)\n        self.model.load_state_dict(net_dict, strict=False)  # Используем strict=False для игнорирования несовпадений\n\n    def forward(self, x):\n        print(f\"Input x shape: {x.shape}\")  # [batch_size, channels, depth, height, width]\n        features = self.model(x)\n        print(f\"Output features shape: {features.shape}\")  # [batch_size, num_classes]\n        return features\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T15:57:20.852789Z","iopub.execute_input":"2024-10-19T15:57:20.853163Z","iopub.status.idle":"2024-10-19T15:57:20.863293Z","shell.execute_reply.started":"2024-10-19T15:57:20.853127Z","shell.execute_reply":"2024-10-19T15:57:20.862476Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"def flatten(xss):\n    return [x for xs in xss for x in xs]","metadata":{"execution":{"iopub.status.busy":"2024-10-19T15:36:05.871962Z","iopub.execute_input":"2024-10-19T15:36:05.872337Z","iopub.status.idle":"2024-10-19T15:36:05.876820Z","shell.execute_reply.started":"2024-10-19T15:36:05.872302Z","shell.execute_reply":"2024-10-19T15:36:05.875856Z"},"trusted":true},"outputs":[],"execution_count":33},{"cell_type":"code","source":"data_transforms = {\n    \"train\": A.Compose([\n        A.augmentations.crops.transforms.CenterCrop(256,256),\n        A.Resize(224,224, interpolation=cv2.INTER_NEAREST),\n        # A.HorizontalFlip(p=0.5),\n        # A.VerticalFlip(p=0.5),\n        A.ShiftScaleRotate(shift_limit=0.0225, scale_limit=0.005, rotate_limit=10, p=1.),\n        A.OneOf([\n            A.GridDistortion(num_steps=5, distort_limit=0.05, p=1.0),\n# #             A.OpticalDistortion(distort_limit=0.05, shift_limit=0.05, p=1.0),\n            A.ElasticTransform(alpha=1, sigma=50, alpha_affine=None, p=1.0)\n        ], p=0.25),\n        A.CoarseDropout(max_holes=8, max_height=224//20, max_width=224//20,\n                         min_holes=5, fill_value=0, mask_fill_value=0, p=1.0),\n        A.augmentations.Normalize(mean=(0.485, ), std=(0.229, )),\n        ]\n        , p=1.0),\n    \n    \"valid\": A.Compose([\n        A.augmentations.crops.transforms.CenterCrop(256,256),\n        A.Resize(224,224, interpolation=cv2.INTER_NEAREST),\n        A.augmentations.Normalize(mean=(0.485, ), std=(0.229, )),\n        ], p=1.0)\n}","metadata":{"execution":{"iopub.status.busy":"2024-10-19T15:36:07.441763Z","iopub.execute_input":"2024-10-19T15:36:07.442400Z","iopub.status.idle":"2024-10-19T15:36:07.453711Z","shell.execute_reply.started":"2024-10-19T15:36:07.442358Z","shell.execute_reply":"2024-10-19T15:36:07.452856Z"},"trusted":true},"outputs":[],"execution_count":34},{"cell_type":"code","source":"x_pth ,y_pth = get_pathes('/kaggle/input/brain-tumor-mri/data_29_05_24_cls108_seg72')\n\nx_pth_train,x_pth_val,y_pth_train,y_pth_val = train_test_split(x_pth , y_pth, test_size=0.2)\nx_pth_train,y_pth_train,x_pth_val,y_pth_val = flatten(x_pth_train) ,flatten(y_pth_train),flatten(x_pth_val),flatten(y_pth_val)","metadata":{"execution":{"iopub.status.busy":"2024-10-19T15:36:09.765805Z","iopub.execute_input":"2024-10-19T15:36:09.766152Z","iopub.status.idle":"2024-10-19T15:36:10.135472Z","shell.execute_reply.started":"2024-10-19T15:36:09.766120Z","shell.execute_reply":"2024-10-19T15:36:10.134655Z"},"trusted":true},"outputs":[],"execution_count":35},{"cell_type":"code","source":"!pip install GPUtil","metadata":{"execution":{"iopub.status.busy":"2024-10-19T15:36:11.568451Z","iopub.execute_input":"2024-10-19T15:36:11.569303Z","iopub.status.idle":"2024-10-19T15:36:25.177138Z","shell.execute_reply.started":"2024-10-19T15:36:11.569253Z","shell.execute_reply":"2024-10-19T15:36:25.176096Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting GPUtil\n  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: GPUtil\n  Building wheel for GPUtil (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7394 sha256=75f61d6e02dead17693aeb1ad12881d469dc0f8f275890891b19bb96dbbcbab7\n  Stored in directory: /root/.cache/pip/wheels/a9/8a/bd/81082387151853ab8b6b3ef33426e98f5cbfebc3c397a9d4d0\nSuccessfully built GPUtil\nInstalling collected packages: GPUtil\nSuccessfully installed GPUtil-1.4.0\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"\nimport GPUtil\n\n\ndef build_model():\n\n    input_dim = 2048  # Размерность эмбеддингов из ResNet50\n    hidden_dim = 2048\n    output_dim = 5  # Количество классов\n    num_layers = 1\n    base_model = MedicalNet(path_to_weights=\"/kaggle/input/medicalnet/Pretrained/Pretrained/resnet_50.pth\", device=CFG.device)\n    \n    # Заморозка параметров\n    for param_name, param in base_model.named_parameters():\n        if param_name.startswith(\"fc1\"):\n            param.requires_grad = True  # Размораживаем только линейные слои\n        else:\n            param.requires_grad = False  # Остальные параметры заморожены\n\n    base_model.to('cuda')\n    return base_model\n\ndef criterion(y_pred, y_true):\n    loss = nn.CrossEntropyLoss()\n    return loss(y_pred, y_true)\n\ndef load_model(path):\n    model = build_model()\n    model.load_state_dict(torch.load(path))\n    model.eval()\n    return model\n\ntrain_dataset = MedicalDataset(x_pth_train,y_pth_train,transform=data_transforms['train'])\nval_dataset = MedicalDataset(x_pth_val,y_pth_val,transform=data_transforms['valid'])\ntrain_dataset_inv = MedicalDatasetInv(x_pth_train,y_pth_train,transform=data_transforms['train'])\nval_dataset_inv  = MedicalDatasetInv(x_pth_val,y_pth_val,transform=data_transforms['valid'])\n\ntrain_norm_inv = torch.utils.data.ConcatDataset([train_dataset, train_dataset_inv])\nval_norm_inv = torch.utils.data.ConcatDataset([val_dataset, val_dataset_inv])\n\ntrain_loader = DataLoader(train_norm_inv, batch_size=4,shuffle=True)\nvalid_loader = DataLoader(val_norm_inv, batch_size=32, shuffle=False)\nfor fold in range(1):\n    \n    print(f'#'*15)\n    print(f'### Fold: {fold}')\n    print(f'#'*15)\n    #run = wandb.init(project='uw-maddison-gi-tract', \n    #                config={k:v for k, v in dict(vars(CFG)).items() if '__' not in k},\n    #                anonymous=anonymous,\n    #                name=f\"fold-{fold}|dim-{CFG.img_size[0]}x{CFG.img_size[1]}|model-{CFG.model_name}\",\n    #                group=CFG.comment,\n    #                )\n    train_loader, valid_loader = train_loader,valid_loader\n    model     =   build_model()\n    # unfreeze_last_n_layers(model, 2)\n\n    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=CFG.lr, weight_decay=CFG.wd)\n    scheduler =  fetch_scheduler(optimizer) \n    # fetch_scheduler(optimizer)\n    \n    model, history = run_training(model, optimizer, scheduler,\n                                device=CFG.device,\n                                num_epochs=CFG.epochs)\n    # run.finish()\n    # display(ipd.IFrame(run.url, width=1000, height=720))\n    plt.figure(figsize=(12,9))\n\n    plt.plot(history['Train Loss'], label=f'Training Loss')\n    plt.plot(history['Valid Loss'], label=f'Validation Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.title(f'Loss History-model')\n\n\n    plt.figure(figsize=(17,9))\n    plt.subplot(1, 2, 1)\n    plt.plot(history['Train Acc'], label=f'Training Acc')\n    plt.plot(history['Valid Acc'], label=f'Validation Acc')\n    plt.xlabel('Epoch')\n    plt.ylabel('Acc')\n    plt.legend()\n    plt.title(f'Acc History-model')\n\n    plt.subplot(1, 2, 2)\n    plt.plot(history['Train Recall'], label=f'Training Recall')\n    plt.plot(history['Valid Recall'], label=f'Validation Recall')\n    plt.xlabel('Epoch')\n    plt.ylabel('Recall')\n    plt.legend()\n    plt.title(f'Recall History-model')\n    plt.show()\n    torch.save(model.state_dict(), 'lstm_bs64_state_dict')\n    gpus = GPUtil.getGPUs()\n    gpu = gpus[0]\n    GPUtil.showUtilization()\n    free_memory = gpu.memoryFree\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-10-19T15:57:26.271303Z","iopub.execute_input":"2024-10-19T15:57:26.271810Z","iopub.status.idle":"2024-10-19T15:57:27.172571Z","shell.execute_reply.started":"2024-10-19T15:57:26.271764Z","shell.execute_reply":"2024-10-19T15:57:27.171347Z"},"trusted":true},"outputs":[{"name":"stdout","text":"###############\n### Fold: 0\n###############\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[52], line 54\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m#run = wandb.init(project='uw-maddison-gi-tract', \u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m#                config={k:v for k, v in dict(vars(CFG)).items() if '__' not in k},\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m#                anonymous=anonymous,\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m#                name=f\"fold-{fold}|dim-{CFG.img_size[0]}x{CFG.img_size[1]}|model-{CFG.model_name}\",\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m#                group=CFG.comment,\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m#                )\u001b[39;00m\n\u001b[1;32m     53\u001b[0m train_loader, valid_loader \u001b[38;5;241m=\u001b[39m train_loader,valid_loader\n\u001b[0;32m---> 54\u001b[0m model     \u001b[38;5;241m=\u001b[39m   \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# unfreeze_last_n_layers(model, 2)\u001b[39;00m\n\u001b[1;32m     57\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m p: p\u001b[38;5;241m.\u001b[39mrequires_grad, model\u001b[38;5;241m.\u001b[39mparameters()), lr\u001b[38;5;241m=\u001b[39mCFG\u001b[38;5;241m.\u001b[39mlr, weight_decay\u001b[38;5;241m=\u001b[39mCFG\u001b[38;5;241m.\u001b[39mwd)\n","Cell \u001b[0;32mIn[52], line 10\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m output_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m  \u001b[38;5;66;03m# Количество классов\u001b[39;00m\n\u001b[1;32m      9\u001b[0m num_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 10\u001b[0m base_model \u001b[38;5;241m=\u001b[39m \u001b[43mMedicalNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_to_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/kaggle/input/medicalnet/Pretrained/Pretrained/resnet_50.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCFG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Заморозка параметров\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param_name, param \u001b[38;5;129;01min\u001b[39;00m base_model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n","Cell \u001b[0;32mIn[51], line 21\u001b[0m, in \u001b[0;36mMedicalNet.__init__\u001b[0;34m(self, path_to_weights, device, num_classes)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mavgpool \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mAdaptiveAvgPool3d((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Получаем количество входных признаков для линейного слоя\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m num_ftrs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_seg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_features\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Заменяем последний линейный слой на новый\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconv_seg \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(num_ftrs, num_classes)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1729\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1727\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1728\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1729\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'in_features'"],"ename":"AttributeError","evalue":"'Sequential' object has no attribute 'in_features'","output_type":"error"}],"execution_count":52},{"cell_type":"code","source":"import GPUtil\nimport torch\n\ngpus = GPUtil.getGPUs()\n\ngpu = gpus[0]\n\nGPUtil.showUtilization()\n\nfree_memory = gpu.memoryFree\n\ntorch.cuda.empty_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T15:00:52.850578Z","iopub.execute_input":"2024-10-12T15:00:52.851024Z","iopub.status.idle":"2024-10-12T15:00:52.911694Z","shell.execute_reply.started":"2024-10-12T15:00:52.850986Z","shell.execute_reply":"2024-10-12T15:00:52.910782Z"}},"outputs":[{"name":"stdout","text":"| ID | GPU | MEM |\n------------------\n|  0 |  0% |  0% |\n|  1 |  0% |  0% |\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"import os\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\ndevice = 'cuda'\n\ndef get_predictions(model, loader):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    with torch.no_grad():\n        for step, (sample) in enumerate(loader):  \n            \n            images, label = sample['image'], sample['labelS']\n    \n            images  = images.to(device).half() \n            label   = label.to(device).long() \n            print(len(label),len(images))\n            batch_size = images.size(0)\n            # batch_features = []\n            # for img in images:\n            #     # Поскольку batch_size=1\n            #         img = torch.permute(img, (1, 0, 2,3))\n            #         img = img.to('cuda').float()\n                    \n            #         features = feature_extractor(img)\n            #         batch_features.append(features)\n                \n            #     # Получаем эмбеддинги для всех срезов\n            # patient_embedding = torch.stack(batch_features).squeeze(1)\n            with amp.autocast(enabled=True):\n                outputs = model(images)\n             \n            if (len(images))==16: \n                print(outputs.size())\n                print(torch.max(outputs,1))\n\n            _,preds = torch.max(outputs,1 )\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(label.cpu().numpy())\n    return all_preds, all_labels\n\n# Получение предсказаний для тренировочного и тестового наборов\ntrain_preds, train_labels = get_predictions(model, train_loader)\ntest_preds, test_labels = get_predictions(model, valid_loader)\nprint(len(train_preds),len(train_labels))\n      # Функция для отрисовки матрицы путаницы\ndef plot_confusion_matrix(preds, labels,filename,title='Confusion Matrix'):\n    cm = confusion_matrix(labels, preds)\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n    disp.plot(cmap='Blues', xticks_rotation='vertical')\n    plt.title(title)\n    plt.savefig(filename)\n    plt.show()\n\n# Отрисовка матрицы путаницы для тренировочного и тестового наборов\n\nplot_confusion_matrix(train_preds, train_labels,  title='Train Confusion Matrix',filename=os.path.join('output', 'train_confusion_matrix.png'))\nplot_confusion_matrix(test_preds, test_labels, title='Test Confusion Matrix',filename=os.path.join('output', 'val_confusion_matrix.png'))","metadata":{},"outputs":[],"execution_count":null}]}